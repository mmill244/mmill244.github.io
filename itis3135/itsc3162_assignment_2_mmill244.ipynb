{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ITCS 3162 - Assignment 2\n",
    "\n",
    "Frequent Itemset Mining - A Priori Algorithm\n",
    "\n",
    "### Name: Madison Miller\n",
    "\n",
    "### Submission instructions\n",
    "\n",
    "- Enter your name in the space above.  \n",
    "- Save your completed json as *itcs3162_assignment_2_**\\<uncc username>**.ipynb*.\n",
    "- Upload **both** the **ipynb** file and the **html** file version of your completed notebook to Canvas.  \n",
    "\n",
    "You can download the notebook in html format by going to *File --> Download as --> HTML*\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining Frequent Hashtags on Twitter\n",
    "\n",
    "For this assignment, you will implement the *a priori* algorithm to mine pairs of hashtags that co-occur frequently in a set of tweets scraped using Twitter's API.\n",
    "\n",
    "The \"baskets\" of hashtags are provided in the included data file **hashtag_baskets.csv**. Please download the file from Canvas and put the file in the **same directory** as this Jupyter notebook.\n",
    "\n",
    "The function provided below will read the data file in as a list of sets of hashtags. Each set of hashtags corresponds to all of the hashtags used in a single tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hashtags():\n",
    "    with open('hashtag_baskets.csv') as f:\n",
    "        return [\n",
    "            set(line.strip().split(','))\n",
    "            for line in f\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hashtag_baskets.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a2c7cafcc7db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhashtag_baskets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_hashtags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhashtag_baskets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Print the first few hashtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-936185c1fd98>\u001b[0m in \u001b[0;36mload_hashtags\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_hashtags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hashtag_baskets.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         return [\n\u001b[1;32m      4\u001b[0m             \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hashtag_baskets.csv'"
     ]
    }
   ],
   "source": [
    "hashtag_baskets = load_hashtags()\n",
    "hashtag_baskets[:10]  # Print the first few hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Frequent items (25 pts)\n",
    "\n",
    "In the cells below:\n",
    "1. Define a function that takes in a *list of item baskets* and a *support threshold*. The function should **return the frequent items**  \n",
    "    (You may also want to return the count of each hashtag along with the tag itself)\n",
    "2. Use your function to find the frequently occurring hashtags in our dataset\n",
    "3. Print out your support threshold and the total number of hashtags that occur frequently\n",
    "4. Print the **top 10** most frequent hashtags sorted by support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequent_items(basket_list, support_threshold):\n",
    "    item_count = {}\n",
    "    result = []\n",
    "    for basket in basket_list:\n",
    "        for item in basket:\n",
    "            if item in item_count.keys():\n",
    "                item_count[item] += 1\n",
    "            else:\n",
    "                item_count[item] = 1\n",
    "    for item, count in item_counts.items():\n",
    "        if count >= support_threshold:\n",
    "            result.append(item)            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_threshold = 10\n",
    "freq_items = frequent_items(hashtag_baskets, support_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(support_threshold)\n",
    "print(freq_items)\n",
    "# print(sum(freq_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for item, support in sorted(freq_items, key=lambda x: x[1]):\n",
    "    while n > 10:\n",
    "        print(item)\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Generate candidates (15 pts)\n",
    "\n",
    "Using your results from Part 1, generate the candidate pairs of hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Frequent pairs (30 pts)\n",
    "\n",
    "1. Using your candidate pairs from Part 2, count the occurrences of each candidate pair in our dataset\n",
    "2. Filter based on your support threshold to find the frequent pairs of hashtags\n",
    "3. Print the **top 10** frequent hashtag pairs sorted by support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Association Rules (30 pts)\n",
    "\n",
    "Using your results from Part 1 and Part 3, find the association rules with high confidence.\n",
    "\n",
    "1. For each frequent pair, derive the **two** association rules from that pair and compute the **confidence** of each rule\n",
    "2. Filter the association rules based on a confidence threshold of your choosing\n",
    "3. Print each association rule and its confidence value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus (5 pts)\n",
    "\n",
    "Repeat the above experiments after applying some text preprocessing to the data\n",
    "1. Convert all text to lowercase\n",
    "2. Remove all punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
